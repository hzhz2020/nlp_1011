{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "random.seed(15)\n",
    "\n",
    "import string\n",
    "#PAD_IDX = 0\n",
    "#UNK_IDX = 1\n",
    "\n",
    "PAD_IDX = int(52948)\n",
    "UNK_IDX = int(77808)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert a sentence into a sequnce of tokens\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [word2idx[token] if token in word2idx else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        #don't remove punctuation\n",
    "        #sample = sample.translate(str.maketrans('','',string.punctuation))\n",
    "        tokens = nltk.word_tokenize(sample.lower())\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, train=False):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total_sample = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    model.eval()\n",
    "    # get a random sample\n",
    "    if train:\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            SNLIDataset(train_sent1_indices, train_sent2_indices, train_label),batch_size=BATCH_SIZE,collate_fn=SNLIvocab_collate_func,\n",
    "            sampler=SubsetRandomSampler(range(10*BATCH_SIZE)))\n",
    "    else:\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            SNLIDataset(val_sent1_indices, val_sent2_indices, val_label),batch_size=BATCH_SIZE,collate_fn=SNLIvocab_collate_func,\n",
    "            sampler=SubsetRandomSampler(range(10*BATCH_SIZE)))\n",
    "\n",
    "    for i, sample in enumerate(loader):\n",
    "            size = sample[0].shape[0]\n",
    "            #print(sample[0].shape)\n",
    "            outputs = F.softmax(model(sample[0], sample[1]), dim=1)\n",
    "            minibatch_loss = criterion(outputs, sample[2])\n",
    "            total_loss += minibatch_loss.item()\n",
    "            predicted = outputs.max(1, keepdim=True)[1].view(-1)\n",
    "            \n",
    "            total_sample += size\n",
    "            label = sample[2]\n",
    "            correct += predicted.eq(label.view_as(predicted)).sum().item()\n",
    "\n",
    "    total_batch = i + 1\n",
    "    acc = 100 * correct / total_sample\n",
    "    los = total_loss / total_batch\n",
    "    return acc, los\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/zh1087/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hw2_data/snli_train.tsv\") as f:\n",
    "    train = f.read().split('\\n')\n",
    "    #print(train)\n",
    "    train_data = [row.split('\\t') for row in train[1:-1]]\n",
    "    train_sent1 = [row[0] for row in train_data]\n",
    "    train_sent2 = [row[1] for row in train_data]\n",
    "    train_label = [int(0) if row[2] == 'entailment' else int(1) if row[2] == 'contradiction' else int(2) for row in train_data]\n",
    "    \n",
    "with open(\"hw2_data/snli_val.tsv\") as f:\n",
    "    val = f.read().split('\\n')\n",
    "    #print(train)\n",
    "    val_data = [row.split('\\t') for row in val[1:-1]]\n",
    "    val_sent1 = [row[0] for row in val_data]\n",
    "    val_sent2 = [row[1] for row in val_data]\n",
    "    val_label = [int(0) if row[2] == 'entailment' else int(1) if row[2] == 'contradiction' else int(2) for row in val_data]\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(train_label, open(\"train_label.p\", \"wb\"))\n",
    "pkl.dump(val_label, open(\"val_label.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_sent1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train data\n",
      "Tokenizing val data\n"
     ]
    }
   ],
   "source": [
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_sent1_tokens, all_train_sent1_tokens = tokenize_dataset(train_sent1)\n",
    "train_sent2_tokens, all_train_sent2_tokens = tokenize_dataset(train_sent2)\n",
    "\n",
    "pkl.dump(train_sent1_tokens, open(\"train_sent1_tokens.p\", \"wb\"))\n",
    "pkl.dump(train_sent2_tokens, open(\"train_sent2_tokens.p\", \"wb\"))\n",
    "\n",
    "# val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_sent1_tokens, _ = tokenize_dataset(val_sent1)\n",
    "val_sent2_tokens, _ = tokenize_dataset(val_sent2)\n",
    "\n",
    "pkl.dump(val_sent1_tokens, open(\"val_sent1_tokens.p\", \"wb\"))\n",
    "pkl.dump(val_sent2_tokens, open(\"val_sent2_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load tokenized data:\n",
    "train_sent1_tokens = pkl.load(open('train_sent1_tokens.p','rb'))\n",
    "train_sent2_tokens = pkl.load(open('train_sent2_tokens.p','rb'))\n",
    "\n",
    "val_sent1_tokens = pkl.load(open('val_sent1_tokens.p','rb'))\n",
    "val_sent2_tokens = pkl.load(open('val_sent2_tokens.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21015"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(all_train_sent1_tokens+all_train_sent2_tokens)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build vocabulary from pretrained embedding matrix, build word2id, id2word, word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#does the dataset contain 'PAD' and 'UNK'\n",
    "all_word_pretrained = [] \n",
    "with open('./wiki-news-300d-1M.vec') as f:\n",
    "    next(f)\n",
    "    all_words_pretrained = []\n",
    "    for i, line in enumerate(f):\n",
    "        line = line.split()\n",
    "        all_word_pretrained.append(line[0])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAD in pretrained: True\n",
      "UNK in pretrained: True\n"
     ]
    }
   ],
   "source": [
    "#check if 'PAD' 'UNK' already have pretrained word embedding\n",
    "print('PAD in pretrained:','PAD' in all_word_pretrained)\n",
    "print('UNK in pretrained:','PAD' in all_word_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find\n",
      "52948\n",
      "find\n",
      "77808\n",
      "PAD_IDX is 52948\n",
      "UNK_IDX is 77808\n"
     ]
    }
   ],
   "source": [
    "words_to_load = 500000\n",
    "all_words = [] #all words for our vocabulary\n",
    "word2idx = {}\n",
    "idx2word = {}\n",
    "word2vec = {}\n",
    "weights_matrix = np.zeros((words_to_load, 300))\n",
    "    \n",
    "with open('./wiki-news-300d-1M.vec') as f:\n",
    "    next(f)\n",
    "    #'PAD', 'UNK' already in pretrained\n",
    "    #randomized a vector representation for PAD\n",
    "    #weights_matrix[0,:] = np.random.normal(scale=0.6, size=(300, ))\n",
    "    #randomized a vector representation for UNK\n",
    "    #weights_matrix[1,:] = np.random.normal(scale=0.6, size=(300, ))\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        line = line.split()\n",
    "        if line[0] == 'PAD':\n",
    "            print('find')\n",
    "            print(i)\n",
    "            PAD_IDX = i\n",
    "        if line[0] == 'UNK':\n",
    "            print('find')\n",
    "            print(i)\n",
    "            UNK_IDX = i\n",
    "        all_words.append(line[0])\n",
    "        weights_matrix[i, :] = np.array(line[1:])\n",
    "        word2idx[line[0]] = i\n",
    "        idx2word[i] = line[0]\n",
    "print('PAD_IDX is {}'.format(PAD_IDX))\n",
    "print('UNK_IDX is {}'.format(UNK_IDX))\n",
    "word2vec = {w: weights_matrix[word2idx[w],:] for w in all_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52948"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx['PAD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.4084,  0.1814,  0.0993, -0.0427,  0.0475,  0.1457, -0.1938,\n",
       "        0.083 , -0.0065,  0.1158,  0.1674, -0.1335,  0.0726, -0.0866,\n",
       "        0.0229,  0.1958,  0.2004,  0.1319,  0.0562, -0.1416, -0.5682,\n",
       "       -0.2068,  0.1826,  0.0426,  0.2019, -0.0824,  0.0769,  0.0712,\n",
       "        0.1564,  0.0922, -0.1993,  0.2104, -0.0782, -0.0809, -0.2806,\n",
       "       -0.2862,  0.1417, -0.022 , -0.002 , -0.1562,  0.1064,  0.0134,\n",
       "        0.0584, -0.0326,  0.0428, -0.0953,  0.087 ,  0.0858, -0.1559,\n",
       "       -0.0927,  0.1273,  0.1616, -0.8534, -0.1217,  0.0891, -0.1072,\n",
       "       -0.1505, -0.0263, -0.0411,  0.0292,  0.089 ,  0.0444, -0.1071,\n",
       "       -0.168 , -0.413 ,  0.1468,  0.1117, -0.0966,  0.0731, -0.1675,\n",
       "       -0.0708,  0.0626,  0.2237, -0.009 , -0.003 ,  0.111 , -0.211 ,\n",
       "        0.1294, -0.0849,  0.0745, -0.0402,  0.0142, -0.0717, -0.1138,\n",
       "       -0.0544, -0.0934,  0.159 , -0.0286,  0.1799,  0.1802, -0.0691,\n",
       "        0.0164, -0.1306, -0.0316,  0.07  ,  0.1684,  0.2396, -0.0592,\n",
       "        0.1818,  0.3708, -0.1132,  0.0159,  0.5832,  0.122 ,  0.1023,\n",
       "        0.1292, -0.0121,  0.0923,  0.0299,  0.139 ,  0.0955,  0.025 ,\n",
       "       -0.0805,  0.2013, -0.09  ,  0.067 , -0.538 , -0.105 ,  0.2164,\n",
       "       -0.0325, -0.0311,  0.2533, -0.0788, -0.1517,  0.0734,  0.1989,\n",
       "        0.1644, -0.0555,  0.0605,  0.1843,  0.1113,  0.1151, -0.0925,\n",
       "       -0.0014, -0.0856, -0.0371, -0.0986,  0.0927,  0.0468,  0.0287,\n",
       "        0.0944, -0.0936,  0.0565,  0.0073, -0.1082, -0.1227, -0.09  ,\n",
       "       -0.1564, -0.1833, -0.3171,  0.0559,  0.3234, -0.0162, -0.1832,\n",
       "       -0.0314,  0.046 , -0.1106,  0.028 ,  0.001 , -0.1037,  0.1131,\n",
       "        0.1855, -0.0522,  0.2258, -0.0286,  0.016 ,  0.1213, -0.039 ,\n",
       "        0.0129,  0.0549, -0.0893,  0.037 ,  0.1785, -0.028 , -0.2003,\n",
       "        0.0216,  0.0722,  0.205 ,  0.0588,  0.0434,  0.1437, -0.0484,\n",
       "       -0.0429, -0.1076,  0.1465,  0.0996,  0.0284,  0.0916, -0.2433,\n",
       "       -0.0544,  0.0153,  0.0066,  0.0801,  0.17  ,  0.1143, -0.1809,\n",
       "       -0.2193, -0.0887, -0.1937, -0.2379,  0.1484, -0.051 , -0.1588,\n",
       "        0.1422,  0.1935,  0.1707, -0.1687, -0.0643,  0.1657, -0.1887,\n",
       "       -0.1548,  0.1063, -0.1428, -0.1781, -0.403 , -0.1498,  0.0756,\n",
       "        0.0682,  0.2034,  0.1617, -0.1235,  0.099 , -0.0745,  0.0042,\n",
       "        0.2088,  0.0534, -0.0094, -0.0708, -0.1432,  0.0447,  0.2618,\n",
       "        0.0229,  0.0361, -0.0023, -0.0627, -0.0368,  0.1537,  0.0241,\n",
       "        0.0481,  0.2574, -0.0821, -0.0438,  0.1771,  0.0799, -0.0414,\n",
       "       -0.1133,  0.1231,  0.1649, -0.1156, -0.4777,  0.0302,  0.038 ,\n",
       "       -0.0768, -0.2671,  0.0658, -0.0858,  0.048 ,  0.2183,  0.0385,\n",
       "        0.0404, -0.0415,  0.0446,  0.16  ,  0.1977, -0.3574,  0.1716,\n",
       "       -0.1577, -0.2656,  0.0042, -0.1689, -0.0262,  0.1546, -0.0345,\n",
       "        0.0317, -0.1992, -0.3254, -0.0065,  0.1887, -0.2482,  0.0467,\n",
       "        0.1398, -0.0497,  0.0181,  0.0894,  0.0156, -0.2656,  0.0641,\n",
       "        0.1297, -0.0796, -0.0918,  0.0656, -0.1251, -0.0844,  0.3013,\n",
       "       -0.2323, -0.1724, -0.1411, -0.1923, -0.3376,  0.1231])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500000, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(all_words, open(\"all_words.p\", \"wb\"))\n",
    "pkl.dump(word2idx, open(\"word2idx.p\", \"wb\"))\n",
    "pkl.dump(idx2word, open(\"idx2word.p\", \"wb\"))\n",
    "pkl.dump(word2vec, open(\"word2vec.p\", \"wb\"))\n",
    "pkl.dump(weights_matrix, open('weights_matrix.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load back data:\n",
    "all_words = pkl.load(open('all_words.p','rb'))\n",
    "word2idx = pkl.load(open('word2idx.p','rb'))\n",
    "idx2word = pkl.load(open('idx2word.p','rb'))\n",
    "word2vec = pkl.load(open('word2vec.p','rb'))\n",
    "weights_matrix = pkl.load(open('weights_matrix.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 109565 ; token Auriemma\n",
      "Token Auriemma; token id 109565\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = random.randint(0, len(idx2word)-1)\n",
    "random_token = idx2word[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, idx2word[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, word2idx[random_token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert token to id in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sent1_indices = token2index_dataset(train_sent1_tokens)\n",
    "train_sent2_indices = token2index_dataset(train_sent2_tokens)\n",
    "\n",
    "val_sent2_indices = token2index_dataset(val_sent2_tokens)\n",
    "val_sent1_indices = token2index_dataset(val_sent1_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train sent1 size is {}\".format(len(train_sent1_indices)))\n",
    "print (\"Train sent2 size is {}\".format(len(train_sent2_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_sent1_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_sent2_indices)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(train_sent1_indices, open(\"train_sent1_indices.p\", \"wb\"))\n",
    "pkl.dump(train_sent2_indices, open(\"train_sent2_indices.p\", \"wb\"))\n",
    "pkl.dump(val_sent2_indices, open(\"val_sent2_indices.p\", \"wb\"))\n",
    "pkl.dump(val_sent1_indices, open(\"val_sent1_indices.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load back all necessary data:\n",
    "all_words = pkl.load(open('all_words.p','rb'))\n",
    "word2idx = pkl.load(open('word2idx.p','rb'))\n",
    "idx2word = pkl.load(open('idx2word.p','rb'))\n",
    "word2vec = pkl.load(open('word2vec.p','rb'))\n",
    "weights_matrix = pkl.load(open('weights_matrix.p','rb'))\n",
    "\n",
    "train_sent1_indices = pkl.load(open(\"train_sent1_indices.p\", \"rb\"))\n",
    "train_sent2_indices = pkl.load(open(\"train_sent2_indices.p\", \"rb\"))\n",
    "val_sent2_indices = pkl.load(open(\"val_sent2_indices.p\", \"rb\"))\n",
    "val_sent1_indices = pkl.load(open(\"val_sent1_indices.p\", \"rb\"))\n",
    "\n",
    "train_label = pkl.load(open(\"train_label.p\", \"rb\"))\n",
    "val_label = pkl.load(open(\"val_label.p\", \"rb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 25\n",
    "class SNLIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sent1_data, sent2_data, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.sent1_data = sent1_data\n",
    "        self.sent2_data = sent2_data\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.sent1_data) == len(self.target_list))\n",
    "        assert (len(self.sent2_data) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        item = dict()\n",
    "        \n",
    "        sent1_index_list = self.sent1_data[key][:MAX_SENTENCE_LENGTH]\n",
    "        sent2_index_list = self.sent2_data[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [sent1_index_list, sent2_index_list, label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52948"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note since PAD is already in dataset, here we need to pad with PAD_IDX not 0\n",
    "def SNLIvocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    sent1_list = []\n",
    "    sent2_list = []\n",
    "    label_list = []\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        \n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec0 = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0 ,MAX_SENTENCE_LENGTH-len(datum[0]))),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        sent1_list.append(list(padded_vec0))\n",
    "    \n",
    "        padded_vec1 = np.pad(np.array(datum[1]),\n",
    "                                pad_width=((0 ,MAX_SENTENCE_LENGTH-len(datum[1]))),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        sent2_list.append(list(padded_vec1))\n",
    "\n",
    "    return [torch.from_numpy(np.array(sent1_list)),torch.from_numpy(np.array(sent2_list)), torch.LongTensor(label_list)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset = SNLIDataset(train_sent1_indices, train_sent2_indices, train_label)\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          collate_fn = SNLIvocab_collate_func,\n",
    "                                          shuffle = True)\n",
    "\n",
    "val_dataset = SNLIDataset(val_sent1_indices, val_sent2_indices, val_label)\n",
    "val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
    "                                        batch_size = BATCH_SIZE,\n",
    "                                        collate_fn = SNLIvocab_collate_func,\n",
    "                                        shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_size, num_layers, num_classes):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights_matrix),freeze=True)\n",
    "        emb_size = weights_matrix.shape[1]\n",
    "        \n",
    "        self.rnn1 = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, dropout=0.5, bidirectional=True)\n",
    "        self.rnn2 = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, dropout=0.5, bidirectional=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size * 2 * 2, 300)\n",
    "        self.dropout1 = nn.Dropout(0.3) \n",
    "        self.out = nn.Linear(300, num_classes)\n",
    "        \n",
    "    \n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(self.num_layers*2, batch_size, self.hidden_size)\n",
    "\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, sent1, sent2):\n",
    "        # reset hidden state\n",
    "\n",
    "        batch_size = BATCH_SIZE\n",
    "\n",
    "        self.hidden1 = self.init_hidden(batch_size)\n",
    "        self.hidden2 = self.init_hidden(batch_size)\n",
    "\n",
    "        # get embedding of characters\n",
    "        sent1_embed = self.embedding(sent1)\n",
    "        sent2_embed = self.embedding(sent2)\n",
    "        \n",
    "        # fprop though RNN\n",
    "        sent1_rnn_out, self.hidden1 = self.rnn1(sent1_embed, self.hidden1)\n",
    "        sent2_rnn_out, self.hidden2 = self.rnn2(sent2_embed, self.hidden2)\n",
    "       \n",
    "    \n",
    "        # sum hidden activations of RNN across time\n",
    "        sent1_rnn_out = torch.sum(sent1_rnn_out, dim=1)\n",
    "        sent2_rnn_out = torch.sum(sent2_rnn_out, dim=1)\n",
    "        \n",
    "        rnn_out = torch.cat([sent1_rnn_out, sent2_rnn_out], 1)\n",
    "        #print(rnn_out.shape)\n",
    "        \n",
    "        x = self.fc1(rnn_out)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        logits = self.out(x)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (embedding): Embedding(500000, 300)\n",
      "  (rnn1): GRU(300, 150, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (rnn2): GRU(300, 150, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (fc1): Linear(in_features=600, out_features=300, bias=True)\n",
      "  (dropout1): Dropout(p=0.3)\n",
      "  (out): Linear(in_features=300, out_features=3, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zh1087/nlp_environment/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = RNN(weights_matrix=weights_matrix, hidden_size=150, num_layers=1, num_classes=3)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "994803"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all trainable parameters:\n",
    "params = sum([np.prod(p.size()) for p in model.parameters()if p.requires_grad ])\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only run 2 epoch for demo purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/1], Step: [301/3125], Validation Acc: 39.375\n",
      "find new record, save the model!\n",
      "Epoch: [1/1], Step: [601/3125], Validation Acc: 41.5625\n",
      "find new record, save the model!\n",
      "Epoch: [1/1], Step: [901/3125], Validation Acc: 40.625\n",
      "Epoch: [1/1], Step: [1201/3125], Validation Acc: 38.75\n",
      "Epoch: [1/1], Step: [1501/3125], Validation Acc: 38.4375\n",
      "Epoch: [1/1], Step: [1801/3125], Validation Acc: 43.125\n",
      "find new record, save the model!\n",
      "Epoch: [1/1], Step: [2101/3125], Validation Acc: 50.3125\n",
      "find new record, save the model!\n",
      "Epoch: [1/1], Step: [2401/3125], Validation Acc: 52.1875\n",
      "find new record, save the model!\n",
      "Epoch: [1/1], Step: [2701/3125], Validation Acc: 52.1875\n",
      "Epoch: [1/1], Step: [3001/3125], Validation Acc: 50.625\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = RNN(weights_matrix=weights_matrix, hidden_size=150, num_layers=1, num_classes=3)\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 1 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "RNNval_accuracy = []\n",
    "RNNval_loss = []\n",
    "RNNtrain_accuracy = []\n",
    "RNNtrain_loss = []\n",
    "best_val_acc = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, sample in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        output = model(sample[0], sample[1])\n",
    "        #print(output)\n",
    "        label = sample[2]\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 300 == 0:\n",
    "            #validate\n",
    "            val_acc, val_los = test_model(model, train=False)\n",
    "            tra_acc, tra_los = test_model(model, train=True)\n",
    "            RNNval_accuracy.append(val_acc)\n",
    "            RNNval_loss.append(val_los)\n",
    "            RNNtrain_accuracy.append(tra_acc)\n",
    "            RNNtrain_loss.append(tra_los)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                print('find new record, save the model!')\n",
    "                torch.save(model.state_dict(), 'RNN150_best_model.pkl')\n",
    "\n",
    "OUT_DICT = {'val_accuracy':RNNval_accuracy, 'val_loss': RNNval_loss, 'train_accuracy':RNNtrain_accuracy, 'train_loss':RNNtrain_loss, 'trainable parameters': sum([np.prod(p.size()) for p in model.parameters()if p.requires_grad ])}\n",
    "\n",
    "pkl.dump(OUT_DICT, open(\"SNL_RNN_HIDDEN150.p\", \"wb\"))               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_size, num_classes,):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.hidden_size =  hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights_matrix),freeze=True)\n",
    "        emb_size = weights_matrix.shape[1]\n",
    "        \n",
    "    \n",
    "        self.sent1_conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.sent1_conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "\n",
    "        self.sent2_conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.sent2_conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_size * 2 , 300)\n",
    "        self.dropout1 = nn.Dropout(0.3) \n",
    "        self.out = nn.Linear(300, num_classes)\n",
    "\n",
    "    def forward(self, sent1, sent2):\n",
    "        batch_size, seq_len = sent1.size()\n",
    "        #print('batch_size1 is {}, batch_size2 is {}'.format(batch_size1, batch_size2))\n",
    "        #print('seq_len1 is {}, seq_len2 is {}'.format(seq_len1,seq_len2))\n",
    "\n",
    "        sent1_embed = self.embedding(sent1)\n",
    "        sent2_embed = self.embedding(sent2)\n",
    "        \n",
    "        sent1_hidden = self.sent1_conv1(sent1_embed.transpose(1,2)).transpose(1,2)\n",
    "        sent1_hidden = F.relu(sent1_hidden.contiguous().view(-1, sent1_hidden.size(-1))).view(batch_size, seq_len, sent1_hidden.size(-1))\n",
    "\n",
    "        sent1_hidden = self.sent1_conv2(sent1_hidden.transpose(1,2)).transpose(1,2)\n",
    "        sent1_hidden = F.relu(sent1_hidden.contiguous().view(-1, sent1_hidden.size(-1))).view(batch_size, seq_len, sent1_hidden.size(-1))\n",
    "\n",
    "        sent1_hidden = torch.sum(sent1_hidden, dim=1)\n",
    "        #print('sent1_hidden shape is {}'.format(sent1_hidden.shape)) #[32, 150]\n",
    "        \n",
    "        sent2_hidden = self.sent2_conv1(sent2_embed.transpose(1,2)).transpose(1,2)\n",
    "        sent2_hidden = F.relu(sent2_hidden.contiguous().view(-1, sent2_hidden.size(-1))).view(batch_size, seq_len, sent2_hidden.size(-1))\n",
    "\n",
    "        sent2_hidden = self.sent2_conv2(sent2_hidden.transpose(1,2)).transpose(1,2)\n",
    "        sent2_hidden = F.relu(sent2_hidden.contiguous().view(-1, sent2_hidden.size(-1))).view(batch_size, seq_len, sent2_hidden.size(-1))\n",
    "\n",
    "        sent2_hidden = torch.sum(sent2_hidden, dim=1)\n",
    "        #print('sent2_hidden shape is {}'.format(sent2_hidden.shape)) #[32, 150]\n",
    "        \n",
    "        cnn_out = torch.cat([sent1_hidden, sent2_hidden], 1)\n",
    "        #print('cnn out shape is {}'.format(cnn_out.shape)) #[32, 300]\n",
    "        \n",
    "        x = self.fc1(cnn_out)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        logits = self.out(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (embedding): Embedding(500000, 300)\n",
      "  (sent1_conv1): Conv1d(300, 150, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (sent1_conv2): Conv1d(150, 150, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (sent2_conv1): Conv1d(300, 150, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (sent2_conv2): Conv1d(150, 150, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (fc1): Linear(in_features=300, out_features=300, bias=True)\n",
      "  (dropout1): Dropout(p=0.3)\n",
      "  (out): Linear(in_features=300, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CNN(weights_matrix=weights_matrix, hidden_size=150,  num_classes=3)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only train 2 epoch for demo purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.CNN'>\n",
      "Epoch: [1/2], Step: [101/3125], Validation Acc: 41.5625\n",
      "find new record, save the model!\n",
      "Epoch: [1/2], Step: [201/3125], Validation Acc: 46.875\n",
      "find new record, save the model!\n",
      "Epoch: [1/2], Step: [301/3125], Validation Acc: 50.9375\n",
      "find new record, save the model!\n",
      "Epoch: [1/2], Step: [401/3125], Validation Acc: 55.3125\n",
      "find new record, save the model!\n",
      "Epoch: [1/2], Step: [501/3125], Validation Acc: 50.9375\n",
      "Epoch: [1/2], Step: [601/3125], Validation Acc: 53.4375\n",
      "Epoch: [1/2], Step: [701/3125], Validation Acc: 54.6875\n",
      "Epoch: [1/2], Step: [801/3125], Validation Acc: 51.5625\n",
      "Epoch: [1/2], Step: [901/3125], Validation Acc: 58.125\n",
      "find new record, save the model!\n",
      "Epoch: [1/2], Step: [1001/3125], Validation Acc: 58.75\n",
      "find new record, save the model!\n",
      "Epoch: [1/2], Step: [1101/3125], Validation Acc: 52.5\n",
      "Epoch: [1/2], Step: [1201/3125], Validation Acc: 56.875\n",
      "Epoch: [1/2], Step: [1301/3125], Validation Acc: 53.75\n",
      "Epoch: [1/2], Step: [1401/3125], Validation Acc: 54.0625\n",
      "Epoch: [1/2], Step: [1501/3125], Validation Acc: 55.9375\n",
      "Epoch: [1/2], Step: [1601/3125], Validation Acc: 59.0625\n",
      "find new record, save the model!\n",
      "Epoch: [1/2], Step: [1701/3125], Validation Acc: 60.0\n",
      "find new record, save the model!\n",
      "Epoch: [1/2], Step: [1801/3125], Validation Acc: 55.625\n",
      "Epoch: [1/2], Step: [1901/3125], Validation Acc: 63.125\n",
      "find new record, save the model!\n",
      "Epoch: [1/2], Step: [2001/3125], Validation Acc: 60.3125\n",
      "Epoch: [1/2], Step: [2101/3125], Validation Acc: 58.75\n",
      "Epoch: [1/2], Step: [2201/3125], Validation Acc: 59.375\n",
      "Epoch: [1/2], Step: [2301/3125], Validation Acc: 60.625\n",
      "Epoch: [1/2], Step: [2401/3125], Validation Acc: 60.0\n",
      "Epoch: [1/2], Step: [2501/3125], Validation Acc: 60.9375\n",
      "Epoch: [1/2], Step: [2601/3125], Validation Acc: 60.3125\n",
      "Epoch: [1/2], Step: [2701/3125], Validation Acc: 60.9375\n",
      "Epoch: [1/2], Step: [2801/3125], Validation Acc: 61.875\n",
      "Epoch: [1/2], Step: [2901/3125], Validation Acc: 58.75\n",
      "Epoch: [1/2], Step: [3001/3125], Validation Acc: 62.5\n",
      "Epoch: [1/2], Step: [3101/3125], Validation Acc: 61.875\n",
      "Epoch: [2/2], Step: [101/3125], Validation Acc: 63.125\n",
      "Epoch: [2/2], Step: [201/3125], Validation Acc: 62.5\n",
      "Epoch: [2/2], Step: [301/3125], Validation Acc: 60.9375\n",
      "Epoch: [2/2], Step: [401/3125], Validation Acc: 61.5625\n",
      "Epoch: [2/2], Step: [501/3125], Validation Acc: 61.25\n",
      "Epoch: [2/2], Step: [601/3125], Validation Acc: 61.875\n",
      "Epoch: [2/2], Step: [701/3125], Validation Acc: 59.375\n",
      "Epoch: [2/2], Step: [801/3125], Validation Acc: 63.4375\n",
      "find new record, save the model!\n",
      "Epoch: [2/2], Step: [901/3125], Validation Acc: 61.25\n",
      "Epoch: [2/2], Step: [1001/3125], Validation Acc: 63.125\n",
      "Epoch: [2/2], Step: [1101/3125], Validation Acc: 63.4375\n",
      "Epoch: [2/2], Step: [1201/3125], Validation Acc: 63.4375\n",
      "Epoch: [2/2], Step: [1301/3125], Validation Acc: 60.9375\n",
      "Epoch: [2/2], Step: [1401/3125], Validation Acc: 63.4375\n",
      "Epoch: [2/2], Step: [1501/3125], Validation Acc: 65.0\n",
      "find new record, save the model!\n",
      "Epoch: [2/2], Step: [1601/3125], Validation Acc: 63.75\n",
      "Epoch: [2/2], Step: [1701/3125], Validation Acc: 60.625\n",
      "Epoch: [2/2], Step: [1801/3125], Validation Acc: 61.25\n",
      "Epoch: [2/2], Step: [1901/3125], Validation Acc: 63.125\n",
      "Epoch: [2/2], Step: [2001/3125], Validation Acc: 63.75\n",
      "Epoch: [2/2], Step: [2101/3125], Validation Acc: 62.1875\n",
      "Epoch: [2/2], Step: [2201/3125], Validation Acc: 62.1875\n",
      "Epoch: [2/2], Step: [2301/3125], Validation Acc: 64.6875\n",
      "Epoch: [2/2], Step: [2401/3125], Validation Acc: 65.3125\n",
      "find new record, save the model!\n",
      "Epoch: [2/2], Step: [2501/3125], Validation Acc: 63.125\n",
      "Epoch: [2/2], Step: [2601/3125], Validation Acc: 62.8125\n",
      "Epoch: [2/2], Step: [2701/3125], Validation Acc: 60.9375\n",
      "Epoch: [2/2], Step: [2801/3125], Validation Acc: 61.25\n",
      "Epoch: [2/2], Step: [2901/3125], Validation Acc: 62.5\n",
      "Epoch: [2/2], Step: [3001/3125], Validation Acc: 63.4375\n",
      "Epoch: [2/2], Step: [3101/3125], Validation Acc: 63.4375\n"
     ]
    }
   ],
   "source": [
    "### CNN:\n",
    "model = CNN(weights_matrix=weights_matrix, hidden_size=150,  num_classes=3)\n",
    "print(type(model))\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 2 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "CNNval_accuracy = []\n",
    "CNNval_loss = []\n",
    "CNNtrain_accuracy = []\n",
    "CNNtrain_loss = []\n",
    "best_val_acc = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, sample in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(sample[0],sample[1])\n",
    "        labels = sample[2]\n",
    "        loss = criterion(outputs, labels)\n",
    "       \n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            val_acc, val_los = test_model(model, train=False)\n",
    "            tra_acc, tra_los = test_model(model, train=True)\n",
    "            CNNval_accuracy.append(val_acc)\n",
    "            CNNval_loss.append(val_los)\n",
    "            CNNtrain_accuracy.append(tra_acc)\n",
    "            CNNtrain_loss.append(tra_los)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                print('find new record, save the model!')\n",
    "                torch.save(model.state_dict(), 'CNN150_best_model.pkl')\n",
    "\n",
    "OUT_DICT = {'val_accuracy':CNNval_accuracy, 'val_loss': CNNval_loss, 'train_accuracy':CNNtrain_accuracy, 'train_loss':CNNtrain_loss, 'trainable parameters': sum([np.prod(p.size()) for p in model.parameters()if p.requires_grad ])}\n",
    "\n",
    "pkl.dump(OUT_DICT, open(\"SNL_CNN_HIDDEN150.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameter tunning1: Different hidden size for RNN and CNN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#script: RNN_hidden150.py\n",
    "         RNN_hidden300.py\n",
    "         RNN_hidden450.py\n",
    "         CNN_hidden150.py\n",
    "         CNN_hidden300.py\n",
    "         CNN_hidden450.py\n",
    "         CNN300_KERNEL.py\n",
    "\n",
    "#visualization: visualize_report.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameter tunning2: Varying CNN kernel size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size_list = [5,7,9]\n",
    "hidden_size = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_kernel(nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_size, kernel_size, num_classes,):\n",
    "\n",
    "        super(CNN_kernel, self).__init__()\n",
    "\n",
    "        self.hidden_size =  hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights_matrix),freeze=True)\n",
    "        emb_size = weights_matrix.shape[1]\n",
    "        \n",
    "    \n",
    "        self.sent1_conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size= kernel_size, padding= int((kernel_size - 1)/2))\n",
    "        self.sent1_conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size= kernel_size, padding= int((kernel_size - 1)/2))\n",
    "\n",
    "        self.sent2_conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size= kernel_size, padding= int((kernel_size - 1)/2))\n",
    "        self.sent2_conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size= kernel_size, padding= int((kernel_size - 1)/2))\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_size * 2 , 300)\n",
    "        self.dropout1 = nn.Dropout(0.3) \n",
    "        self.out = nn.Linear(300, num_classes)\n",
    "\n",
    "    def forward(self, sent1, sent2):\n",
    "        batch_size, seq_len = sent1.size()\n",
    "        #print('batch_size1 is {}, batch_size2 is {}'.format(batch_size1, batch_size2))\n",
    "        #print('seq_len1 is {}, seq_len2 is {}'.format(seq_len1,seq_len2))\n",
    "\n",
    "        sent1_embed = self.embedding(sent1) #[32, 25, 300]\n",
    "        sent2_embed = self.embedding(sent2)\n",
    "        #print('sent1_embed shape {}'.format(sent1_embed.size()))\n",
    "        #print('sent2_embed shape {}'.format(sent2_embed.size()))\n",
    "        \n",
    "        sent1_hidden = self.sent1_conv1(sent1_embed.transpose(1,2)).transpose(1,2) #[32, 25, 150]\n",
    "        #print('shape is {}'.format(sent1_hidden.size()))\n",
    "        sent1_hidden = F.relu(sent1_hidden.contiguous().view(-1, sent1_hidden.size(-1))).view(batch_size, seq_len, -1) #[32, 25, 150]\n",
    "        #print('shape is {}'.format(sent1_hidden.size()))\n",
    "\n",
    "        sent1_hidden = self.sent1_conv2(sent1_hidden.transpose(1,2)).transpose(1,2)#[32, 25, 150]\n",
    "        #print('shape is {}'.format(sent1_hidden.size()))\n",
    "\n",
    "        sent1_hidden = F.relu(sent1_hidden.contiguous().view(-1, sent1_hidden.size(-1))).view(batch_size, seq_len, -1)#[32, 25, 150]\n",
    "        #print('shape is {}'.format(sent1_hidden.size()))\n",
    "\n",
    "        sent1_hidden = torch.sum(sent1_hidden, dim=1)\n",
    "        #print('sent1_hidden shape is {}'.format(sent1_hidden.shape)) #[32, 150]\n",
    "        \n",
    "        sent2_hidden = self.sent2_conv1(sent2_embed.transpose(1,2)).transpose(1,2)\n",
    "        sent2_hidden = F.relu(sent2_hidden.contiguous().view(-1, sent2_hidden.size(-1))).view(batch_size, seq_len, -1)\n",
    "\n",
    "        sent2_hidden = self.sent2_conv2(sent2_hidden.transpose(1,2)).transpose(1,2)\n",
    "        sent2_hidden = F.relu(sent2_hidden.contiguous().view(-1, sent2_hidden.size(-1))).view(batch_size, seq_len, -1)\n",
    "\n",
    "        sent2_hidden = torch.sum(sent2_hidden, dim=1)\n",
    "        #print('sent2_hidden shape is {}'.format(sent2_hidden.shape)) #[32, 150]\n",
    "        \n",
    "        cnn_out = torch.cat([sent1_hidden, sent2_hidden], 1)\n",
    "        #print('cnn out shape is {}'.format(cnn_out.shape)) #[32, 300]\n",
    "        \n",
    "        x = self.fc1(cnn_out)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        logits = self.out(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### each run 2 epoch, for demo purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/1], Step: [301/3125], Validation Acc: 52.1875\n",
      "find new record, save the model!\n",
      "Epoch: [1/1], Step: [601/3125], Validation Acc: 53.75\n",
      "find new record, save the model!\n",
      "Epoch: [1/1], Step: [901/3125], Validation Acc: 55.0\n",
      "find new record, save the model!\n",
      "Epoch: [1/1], Step: [1201/3125], Validation Acc: 58.75\n",
      "find new record, save the model!\n",
      "Epoch: [1/1], Step: [1501/3125], Validation Acc: 55.625\n",
      "Epoch: [1/1], Step: [1801/3125], Validation Acc: 55.625\n",
      "Epoch: [1/1], Step: [2101/3125], Validation Acc: 56.5625\n",
      "Epoch: [1/1], Step: [2401/3125], Validation Acc: 60.3125\n",
      "find new record, save the model!\n",
      "Epoch: [1/1], Step: [2701/3125], Validation Acc: 62.1875\n",
      "find new record, save the model!\n",
      "Epoch: [1/1], Step: [3001/3125], Validation Acc: 62.8125\n",
      "find new record, save the model!\n",
      "Epoch: [1/1], Step: [301/3125], Validation Acc: 51.25\n",
      "find new record, save the model!\n",
      "Epoch: [1/1], Step: [601/3125], Validation Acc: 51.875\n",
      "find new record, save the model!\n",
      "Epoch: [1/1], Step: [901/3125], Validation Acc: 56.25\n",
      "find new record, save the model!\n",
      "Epoch: [1/1], Step: [1201/3125], Validation Acc: 53.75\n",
      "Epoch: [1/1], Step: [1501/3125], Validation Acc: 60.0\n",
      "find new record, save the model!\n",
      "Epoch: [1/1], Step: [1801/3125], Validation Acc: 61.25\n",
      "find new record, save the model!\n",
      "Epoch: [1/1], Step: [2101/3125], Validation Acc: 59.375\n",
      "Epoch: [1/1], Step: [2401/3125], Validation Acc: 57.8125\n",
      "Epoch: [1/1], Step: [2701/3125], Validation Acc: 59.0625\n",
      "Epoch: [1/1], Step: [3001/3125], Validation Acc: 59.6875\n",
      "Epoch: [1/1], Step: [301/3125], Validation Acc: 50.0\n",
      "find new record, save the model!\n",
      "Epoch: [1/1], Step: [601/3125], Validation Acc: 51.25\n",
      "find new record, save the model!\n",
      "Epoch: [1/1], Step: [901/3125], Validation Acc: 54.6875\n",
      "find new record, save the model!\n",
      "Epoch: [1/1], Step: [1201/3125], Validation Acc: 58.125\n",
      "find new record, save the model!\n",
      "Epoch: [1/1], Step: [1501/3125], Validation Acc: 56.5625\n",
      "Epoch: [1/1], Step: [1801/3125], Validation Acc: 52.5\n",
      "Epoch: [1/1], Step: [2101/3125], Validation Acc: 53.125\n",
      "Epoch: [1/1], Step: [2401/3125], Validation Acc: 60.625\n",
      "find new record, save the model!\n",
      "Epoch: [1/1], Step: [2701/3125], Validation Acc: 56.5625\n",
      "Epoch: [1/1], Step: [3001/3125], Validation Acc: 58.4375\n"
     ]
    }
   ],
   "source": [
    "for kernel_size in kernel_size_list:     \n",
    "    ### CNN:\n",
    "    model = CNN_kernel(weights_matrix=weights_matrix, hidden_size=150, kernel_size= kernel_size, num_classes=3)\n",
    "    learning_rate = 3e-4\n",
    "    num_epochs = 1 # number epoch to train\n",
    "\n",
    "    # Criterion and Optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    CNNval_accuracy = []\n",
    "    CNNval_loss = []\n",
    "    CNNtrain_accuracy = []\n",
    "    CNNtrain_loss = []\n",
    "    best_val_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, sample in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(sample[0],sample[1])\n",
    "            labels = sample[2]\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 300 == 0:\n",
    "                val_acc, val_los = test_model(model, train=False)\n",
    "                tra_acc, tra_los = test_model(model, train=True)\n",
    "                CNNval_accuracy.append(val_acc)\n",
    "                CNNval_loss.append(val_los)\n",
    "                CNNtrain_accuracy.append(tra_acc)\n",
    "                CNNtrain_loss.append(tra_los)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    print('find new record, save the model!')\n",
    "                    model_name = 'CNN150_KERNEL' + str(kernel_size) + '_best_model.pkl' \n",
    "                    torch.save(model.state_dict(), model_name)\n",
    "\n",
    "    OUT_DICT = {'val_accuracy':CNNval_accuracy, 'val_loss': CNNval_loss, 'train_accuracy':CNNtrain_accuracy, 'train_loss':CNNtrain_loss, 'trainable parameters': sum([np.prod(p.size()) for p in model.parameters()if p.requires_grad ])}\n",
    "    OUT_DICT_NAME = 'SNL_CNN_HIDDEN150_KERNEL' + str(kernel_size) + '.p'\n",
    "    pkl.dump(OUT_DICT, open(OUT_DICT_NAME, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report 3 correct and 3 Incorrect example for the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zh1087/nlp_environment/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "#load the model for testing\n",
    "#example:\n",
    "\n",
    "best_model = RNN(weights_matrix=weights_matrix, hidden_size=300, num_layers=1, num_classes=3)\n",
    "best_model.load_state_dict(torch.load('./parameters_tunning/RNN300_best_model.pkl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true label_batch is tensor([1, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0, 0, 2, 1, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0,\n",
      "        1, 2, 1, 0, 0, 2, 2, 2])\n",
      "predicted labels is tensor([0, 0, 0, 0, 0, 1, 0, 2, 2, 0, 0, 0, 2, 0, 0, 2, 2, 1, 2, 0, 0, 2, 0, 1,\n",
      "        1, 2, 0, 0, 0, 2, 2, 2])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "#print 3 correct and 3 incorrect prediction of the final model on validation set\n",
    "count = 0\n",
    "for sample in val_loader:\n",
    "    if count == 1:\n",
    "        break\n",
    "    sent1_batch, sent2_batch, label_batch = sample[0], sample[1], sample[2]\n",
    "    #print('sent1_batch is {}'.format(sent1_batch[0]))\n",
    "    #print('sent2_batch is {}'.format(sent2_batch[0]))\n",
    "    print('true label_batch is {}'.format(label_batch))\n",
    "    \n",
    "    outputs = F.softmax(best_model(sent1_batch, sent2_batch), dim=1)\n",
    "    predicted = outputs.max(1, keepdim=True)[1]\n",
    "    print('predicted labels is {}'.format(predicted.view_as(label_batch)))\n",
    "    \n",
    "    print((predicted.view_as(label_batch)!=label_batch))\n",
    "    \n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_example = [5, 7, 9]\n",
    "correct_example = [2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example:\n",
      "true label is 0 predicted to be 1\n",
      "\n",
      "\n",
      "sent1 is: two people are in a green forest . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "sent2 is: the forest is not dead . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "\n",
      "\n",
      "\n",
      "example:\n",
      "true label is 1 predicted to be 2\n",
      "\n",
      "\n",
      "sent1 is: two women , one walking her dog the other pushing a stroller . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "sent2 is: there is a snowstorm . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "\n",
      "\n",
      "\n",
      "example:\n",
      "true label is 1 predicted to be 0\n",
      "\n",
      "\n",
      "sent1 is: three people and a white dog are sitting in the sand on a beach . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "sent2 is: three dogs and a person are sitting in the snow . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 0: entailmewnt, 1: contradiction, 2: Neutral\n",
    "for i in incorrect_example:\n",
    "    sent1_word = ' '.join([idx2word[j.item()] for j in sent1_batch[i]])\n",
    "    sent2_word = ' '.join([idx2word[j.item()] for j in sent2_batch[i]])\n",
    "    print('example:')\n",
    "    print('true label is {} predicted to be {}'.format(label_batch[i],predicted[i].data[0]))\n",
    "    print('\\n')\n",
    "    print('sent1 is: {}'.format(sent1_word))\n",
    "    print('sent2 is: {}'.format(sent2_word))\n",
    "    print('')\n",
    "    #print(' '.join(sent1_word))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example:\n",
      "true label is 0 predicted to be 0\n",
      "\n",
      "\n",
      "sent1 is: bicycles stationed while a group of people socialize . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "sent2 is: people get together near a stand of bicycles . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "\n",
      "\n",
      "\n",
      "example:\n",
      "true label is 0 predicted to be 0\n",
      "\n",
      "\n",
      "sent1 is: man in overalls with two horses . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "sent2 is: a man in overalls with two horses PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "\n",
      "\n",
      "\n",
      "example:\n",
      "true label is 0 predicted to be 0\n",
      "\n",
      "\n",
      "sent1 is: man observes a wavelength given off by an electronic device . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "sent2 is: the man is examining what wavelength is given off by the device . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 0: entailmewnt, 1: contradiction, 2: Neutral\n",
    "for i in correct_example:\n",
    "    sent1_word = ' '.join([idx2word[j.item()] for j in sent1_batch[i]])\n",
    "    sent2_word = ' '.join([idx2word[j.item()] for j in sent2_batch[i]])\n",
    "    print('example:')\n",
    "    print('true label is {} predicted to be {}'.format(label_batch[i],predicted[i].data[0]))\n",
    "    print('\\n')\n",
    "    print('sent1 is: {}'.format(sent1_word))\n",
    "    print('sent2 is: {}'.format(sent2_word))\n",
    "    print('')\n",
    "    #print(' '.join(sent1_word))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART2: MultiNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#genre_data = {}\n",
    "with open(\"hw2_data/mnli_val.tsv\") as f:\n",
    "    val = f.read().split('\\n')\n",
    "    #print(train)\n",
    "    MNLval_data = [row.split('\\t') for row in val[1:-1]]\n",
    "    MNLval_sent1 = [row[0] for row in MNLval_data]\n",
    "    MNLval_sent2 = [row[1] for row in MNLval_data]\n",
    "    MNLval_label = [int(0) if row[2] == 'entailment' else int(1) if row[2] == 'contradiction' else int(2) for row in MNLval_data]\n",
    "    MNLval_genre = [row[3] for row in MNLval_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(MNLval_label, open(\"MNLval_label.p\", \"wb\"))\n",
    "pkl.dump(MNLval_genre, open(\"MNLval_genre.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fiction', 'government', 'slate', 'telephone', 'travel'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(MNLval_genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing MNLIval data\n"
     ]
    }
   ],
   "source": [
    "# val set tokens\n",
    "print (\"Tokenizing MNLIval data\")\n",
    "MNLval_sent1_tokens, _ = tokenize_dataset(MNLval_sent1)\n",
    "MNLval_sent2_tokens, _ = tokenize_dataset(MNLval_sent2)\n",
    "\n",
    "pkl.dump(MNLval_sent1_tokens, open(\"MNLval_sent1_tokens.p\", \"wb\"))\n",
    "pkl.dump(MNLval_sent2_tokens, open(\"MNLval_sent2_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert token to id in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNLVal dataset size is 5000\n",
      "MNLVal dataset size is 5000\n"
     ]
    }
   ],
   "source": [
    "MNLval_sent2_indices = token2index_dataset(MNLval_sent2_tokens)\n",
    "MNLval_sent1_indices = token2index_dataset(MNLval_sent1_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"MNLVal dataset size is {}\".format(len(MNLval_sent1_indices)))\n",
    "print (\"MNLVal dataset size is {}\".format(len(MNLval_sent2_indices)))\n",
    "\n",
    "pkl.dump(MNLval_sent2_indices, open(\"MNLval_sent2_indices.p\", \"wb\"))\n",
    "pkl.dump(MNLval_sent1_indices, open(\"MNLval_sent1_indices.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load back all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load back all necessary data:\n",
    "all_words = pkl.load(open('all_words.p','rb'))\n",
    "word2idx = pkl.load(open('word2idx.p','rb'))\n",
    "idx2word = pkl.load(open('idx2word.p','rb'))\n",
    "word2vec = pkl.load(open('word2vec.p','rb'))\n",
    "weights_matrix = pkl.load(open('weights_matrix.p','rb'))\n",
    "\n",
    "MNLval_sent2_indices = pkl.load(open(\"MNLval_sent2_indices.p\", \"rb\"))\n",
    "MNLval_sent1_indices = pkl.load(open(\"MNLval_sent1_indices.p\", \"rb\"))\n",
    "\n",
    "MNLval_label = pkl.load(open(\"MNLval_label.p\", \"rb\"))\n",
    "MNLval_genre = pkl.load(open(\"MNLval_genre.p\", \"rb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 25\n",
    "class MNLDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sent1_data, sent2_data, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.sent1_data = sent1_data\n",
    "        self.sent2_data = sent2_data\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.sent1_data) == len(self.target_list))\n",
    "        assert (len(self.sent2_data) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        item = dict()\n",
    "        \n",
    "        sent1_index_list = self.sent1_data[key][:MAX_SENTENCE_LENGTH]\n",
    "        sent2_index_list = self.sent2_data[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [sent1_index_list, sent2_index_list, label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note since PAD is already in dataset, here we need to pad with PAD_IDX not 0\n",
    "def MNLvocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    sent1_list = []\n",
    "    sent2_list = []\n",
    "    label_list = []\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        \n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec0 = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0 ,MAX_SENTENCE_LENGTH-len(datum[0]))),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        sent1_list.append(list(padded_vec0))\n",
    "    \n",
    "        padded_vec1 = np.pad(np.array(datum[1]),\n",
    "                                pad_width=((0 ,MAX_SENTENCE_LENGTH-len(datum[1]))),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        sent2_list.append(list(padded_vec1))\n",
    "\n",
    "    return [torch.from_numpy(np.array(sent1_list)),torch.from_numpy(np.array(sent2_list)), torch.LongTensor(label_list)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['government', 'travel', 'slate', 'fiction', 'telephone']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(MNLval_genre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False, ..., False, False,  True])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.array(MNLval_genre)=='fiction'\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Best CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load best model for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = CNN_kernel(weights_matrix=weights_matrix, hidden_size=300, kernel_size= 3, num_classes=3)\n",
    "best_model.load_state_dict(torch.load('./parameters_tunning/CNN300_best_model.pkl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current genre is slate\n",
      "current genre is travel\n",
      "current genre is telephone\n",
      "current genre is fiction\n",
      "current genre is government\n"
     ]
    }
   ],
   "source": [
    "result_dict = {}\n",
    "for i in list(set(MNLval_genre)):\n",
    "    print('current genre is {}'.format(i))\n",
    "    mask = np.array(MNLval_genre)== i\n",
    "    \n",
    "    genre_sent1_indices = np.array(MNLval_sent2_indices)[mask]\n",
    "    genre_sent2_indices = np.array(MNLval_sent2_indices)[mask]\n",
    "    genre_label = np.array(MNLval_label)[mask]\n",
    "    \n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    MNLval_dataset = MNLDataset(genre_sent1_indices, genre_sent2_indices, genre_label)\n",
    "    MNLval_loader = torch.utils.data.DataLoader(dataset = MNLval_dataset,\n",
    "                                        batch_size = BATCH_SIZE,\n",
    "                                        collate_fn = MNLvocab_collate_func,\n",
    "                                        shuffle = False)\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_sample = 0 \n",
    "    \n",
    "    for sample in MNLval_loader:\n",
    "        sent1_batch, sent2_batch, label_batch = sample[0], sample[1], sample[2]\n",
    "        #print('sent1_batch is {}'.format(sent1_batch[0]))\n",
    "        #print('sent2_batch is {}'.format(sent2_batch[0]))\n",
    "        #print('label_batch is {}'.format(label_batch))\n",
    "        sample_size = label_batch.shape[0]\n",
    "        #print(sample_size)\n",
    "        \n",
    "        outputs = F.softmax(best_model(sent1_batch, sent2_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        #print('predicted labels is {}'.format(predicted.view_as(label_batch)))\n",
    "        \n",
    "        #print((predicted.view_as(label_batch)!=label_batch))\n",
    "        correct = np.sum((predicted.view_as(label_batch)!=label_batch).data.numpy(),axis = 0)\n",
    "        #print(correct)\n",
    "\n",
    "        total_correct += correct\n",
    "        #print(total_correct)\n",
    "        total_sample += sample_size\n",
    "        #print(total_sample)\n",
    "    \n",
    "    MNLval_acc = 100 * total_correct/total_sample\n",
    "    result_dict[i] = MNLval_acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST CNN validation accuracy for fiction is 58.19095477386934\n",
      "BEST CNN validation accuracy for government is 53.74015748031496\n",
      "BEST CNN validation accuracy for slate is 59.98003992015968\n",
      "BEST CNN validation accuracy for telephone is 60.09950248756219\n",
      "BEST CNN validation accuracy for travel is 54.58248472505092\n"
     ]
    }
   ],
   "source": [
    "print('BEST CNN validation accuracy for {} is {}'.format('fiction',result_dict['fiction']))\n",
    "print('BEST CNN validation accuracy for {} is {}'.format('government',result_dict['government']))\n",
    "print('BEST CNN validation accuracy for {} is {}'.format('slate',result_dict['slate']))\n",
    "print('BEST CNN validation accuracy for {} is {}'.format('telephone',result_dict['telephone']))\n",
    "print('BEST CNN validation accuracy for {} is {}'.format('travel',result_dict['travel']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Best RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load best model for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model  = RNN(weights_matrix=weights_matrix, hidden_size=300, num_layers=1, num_classes=3)\n",
    "best_model.load_state_dict(torch.load('./parameters_tunning/RNN300_best_model.pkl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current genre is telephone\n",
      "current genre is travel\n",
      "current genre is fiction\n",
      "current genre is government\n",
      "current genre is slate\n"
     ]
    }
   ],
   "source": [
    "result_dict = {}\n",
    "for i in list(set(MNLval_genre)):\n",
    "    print('current genre is {}'.format(i))\n",
    "    mask = np.array(MNLval_genre)== i\n",
    "    \n",
    "    genre_sent1_indices = np.array(MNLval_sent2_indices)[mask]\n",
    "    genre_sent2_indices = np.array(MNLval_sent2_indices)[mask]\n",
    "    genre_label = np.array(MNLval_label)[mask]\n",
    "    \n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    MNLval_dataset = MNLDataset(genre_sent1_indices, genre_sent2_indices, genre_label)\n",
    "    MNLval_loader = torch.utils.data.DataLoader(dataset = MNLval_dataset,\n",
    "                                        batch_size = BATCH_SIZE,\n",
    "                                        collate_fn = MNLvocab_collate_func,\n",
    "                                        shuffle = False,sampler=SubsetRandomSampler(range(10*BATCH_SIZE)))\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_sample = 0 \n",
    "    \n",
    "    for sample in MNLval_loader:\n",
    "        sent1_batch, sent2_batch, label_batch = sample[0], sample[1], sample[2]\n",
    "        #print('sent1_batch is {}'.format(sent1_batch[0]))\n",
    "        #print('sent2_batch is {}'.format(sent2_batch[0]))\n",
    "        #print('label_batch is {}'.format(label_batch))\n",
    "        sample_size = label_batch.shape[0]\n",
    "        #print(sample_size)\n",
    "        #output = model(sample[0], sample[1])\n",
    "        outputs = F.softmax(best_model(sent1_batch, sent2_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        #print('predicted labels is {}'.format(predicted.view_as(label_batch)))\n",
    "        \n",
    "        #print((predicted.view_as(label_batch)!=label_batch))\n",
    "        correct = np.sum((predicted.view_as(label_batch)!=label_batch).data.numpy(),axis = 0)\n",
    "        #print(correct)\n",
    "\n",
    "        total_correct += correct\n",
    "        #print(total_correct)\n",
    "        total_sample += sample_size\n",
    "        #print(total_sample)\n",
    "    \n",
    "    MNLval_acc = 100 * total_correct/total_sample\n",
    "    result_dict[i] = MNLval_acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST RNN validation accuracy for fiction is 60.0\n",
      "BEST RNN validation accuracy for government is 56.875\n",
      "BEST RNN validation accuracy for slate is 56.875\n",
      "BEST RNN validation accuracy for telephone is 53.4375\n",
      "BEST RNN validation accuracy for travel is 59.6875\n"
     ]
    }
   ],
   "source": [
    "print('BEST RNN validation accuracy for {} is {}'.format('fiction',result_dict['fiction']))\n",
    "print('BEST RNN validation accuracy for {} is {}'.format('government',result_dict['government']))\n",
    "print('BEST RNN validation accuracy for {} is {}'.format('slate',result_dict['slate']))\n",
    "print('BEST RNN validation accuracy for {} is {}'.format('telephone',result_dict['telephone']))\n",
    "print('BEST RNN validation accuracy for {} is {}'.format('travel',result_dict['travel']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
